From J.Demeyer at UGent.be  Mon Apr  1 01:31:03 2019
From: J.Demeyer at UGent.be (Jeroen Demeyer)
Date: Mon, 1 Apr 2019 07:31:03 +0200
Subject: [Python-Dev] PEP 580/590 discussion
In-Reply-To: <5C9FEF82.50207@UGent.be>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
Message-ID: <5CA1A217.1030007@UGent.be>

I added benchmarks for PEP 590:

https://gist.github.com/jdemeyer/f0d63be8f30dc34cc989cd11d43df248

From songofacandy at gmail.com  Mon Apr  1 04:26:31 2019
From: songofacandy at gmail.com (Inada Naoki)
Date: Mon, 1 Apr 2019 17:26:31 +0900
Subject: [Python-Dev] Removing PendingDeprecationWarning
In-Reply-To: <CADiSq7etd9fs2OmmoWMYzuzLxe793PpmCd=8Mrv+tdmFjwdZ=Q@mail.gmail.com>
References: <CAEfz+Tzht97YUsYXFjErc4uKP3J89Yx2ycqhbfk=OkY0w37Yog@mail.gmail.com>
 <CADiSq7d-01UmD9tDg-VEotMGpUSpXgbH8TuuDQFCKWi7jC-39Q@mail.gmail.com>
 <CAEfz+Tzak3XDzSqV=JwWOZN4MkEK8RCTh7rQuNZijVbZ3pE_Aw@mail.gmail.com>
 <CADiSq7eagL+3ZSPvveUocF9-3OB5B_ryyH0DNMTZD72sOJoG7w@mail.gmail.com>
 <CAEfz+Tzj5uh3K9yOF-k+1S1_s_32CSz2_Ad4kB6WH+Y7m_c1BA@mail.gmail.com>
 <CAEfz+TxwTgN=cu0t1OoKqNdJTSrNpvYkVvqUoTHOsE4RgQA9OA@mail.gmail.com>
 <18ccdacf-8fc8-5130-b4ba-89df84e02987@python.org>
 <CAEfz+TytJj=oMXKUE5T8dJj0Jq6ZRp6Uex-JSwmpSaOt39Mx2Q@mail.gmail.com>
 <CADiSq7etd9fs2OmmoWMYzuzLxe793PpmCd=8Mrv+tdmFjwdZ=Q@mail.gmail.com>
Message-ID: <CAEfz+TzAv_oHVk0k2Bnh1KKETCdG=u63kR=bZrsCt2WT+e+QEQ@mail.gmail.com>

On Sat, Mar 30, 2019 at 7:31 PM Nick Coghlan <ncoghlan at gmail.com> wrote:
>
> That's just a documentation fix: "If you're not sure whether to use
> DeprecationWarning or PendingDeprecationWarning, use
> DeprecationWarning".
>

Current proposed patch is:

"""
.. note::
   PendingDeprecationWarning was introduced as an "ignored by default"
   version of DeprecationWarning. But :exc:`DeprecationWarning` is also
   ignored by default since Python 2.7 and 3.2.
   There is not much difference between PendingDeprecationWarning and
   DeprecationWarning nowadays. DeprecationWarning is recommended
   in general.
"""

https://github.com/python/cpython/pull/12505/files#diff-4d7187c7266c3f79727d358de3b3d228

-- 
Inada Naoki  <songofacandy at gmail.com>

From steve.dower at python.org  Mon Apr  1 12:12:26 2019
From: steve.dower at python.org (Steve Dower)
Date: Mon, 1 Apr 2019 09:12:26 -0700
Subject: [Python-Dev] Strange umask(?)/st_mode issue
In-Reply-To: <CAGE7PNLeN1bRFMQMuQ6AFrYYwEzcPYVDhy5hXG-FbRju5FDpCA@mail.gmail.com>
References: <13f98061-6f64-2e8b-de66-d84a7be00a17@python.org>
 <20190330023947.GA62291@cskk.homeip.net>
 <55e6f051-179d-73b4-4cca-b91c5c81b498@python.org>
 <3210441d-6094-b53e-6bf7-4b7c4cfb16ea@python.org>
 <CAGE7PNLeN1bRFMQMuQ6AFrYYwEzcPYVDhy5hXG-FbRju5FDpCA@mail.gmail.com>
Message-ID: <c478fae6-349e-022e-3b70-81470c0a0b5a@python.org>

On 30Mar2019 1130, Gregory P. Smith wrote:
> I wouldn't expect it to be the case in a CI environment but I believe a 
> umask can be overridden if the filesystem is mounted and configured with 
> acls set?? (oh, hah, Ivan just said the same thing)

Yep, it appears this is the case. The Pipelines team got back to me and 
it seems to be a known issue - the workaround they gave me was to run 
"sudo setfacl -Rb /home/vsts" at the start, so I've merged that in for 
now (to master and 3.7).

Cheers,
Steve

From steve.dower at python.org  Mon Apr  1 12:31:36 2019
From: steve.dower at python.org (Steve Dower)
Date: Mon, 1 Apr 2019 09:31:36 -0700
Subject: [Python-Dev] PEP 578: Python Runtime Audit Hooks
In-Reply-To: <8466c9c4-b5dc-c6c5-6fe4-a49dc2f4f968@python.org>
References: <daa13e96-fa07-8703-4d73-234a862baf05@python.org>
 <2cb3740e-ebc2-1839-1d2e-73d1b9f0a445@python.org>
 <ad05e257-d046-bba8-b070-c24c80a1521e@python.org>
 <8466c9c4-b5dc-c6c5-6fe4-a49dc2f4f968@python.org>
Message-ID: <f87504d1-4cc0-e74c-d32e-1e06cde29c39@python.org>

On 31Mar2019 0538, Christian Heimes wrote:
> I don't like the fact that the PEP requires users to learn and use an
> additional layer to handle native code. Although we cannot provide a
> fully secure hook for native code, we could at least try to provide a
> best effort hook and document the limitations. A bit more information
> would make the verified open function more useful, too.

So instead they need to learn a significantly more complicated API? :) 
(I was very happy to be able to say "it's the same as open(p, 'rb')").

> PyObject *PyImport_OpenForExecution(
>      const char *path,
>      const char *intent,
>      int flags,
>      PyObject *context
> )
> 
> - Path is an absolute (!) file path. The PEP doesn't specify if the file
> name is relative or absolute. IMO it should be always absolute.

Yeah, this is fair enough. I'll add it as a requirement.

> - The new intent argument lets the caller pass information how it
> intents to use the file, e.g. pythoncode, zipimport, nativecode (for
> loading a shared library/DLL), ctypes, ... This allows the verify hook
> to react on the intent and provide different verifications for e.g.
> Python code and native modules.

I had an intent argument at one point and the feedback I got (from teams 
who wanted to implement it) is that they wouldn't trust it anyway :)

In each case there should be associated audit events for tracking the 
intent (and interrupting at that point if it doesn't like the intended 
action), but for the simple case of "let me open this specific file" it 
doesn't really add much. And it almost certainly shouldn't impact 
decision making.

> - The flags argument is for additional flags, e.g. return an opened file
> or None, open the file in text or binary mode, ...

This just makes it harder for the hook implementer - now you have to 
allow encoding/errors arguments and probably more. And as mentioned 
above, there should be an audit event showing the intent before this 
call, and a hook can reject it at that point (rather than verify without 
actually returning the verified content).

> - Context is an optional Python object from the caller's context. For
> the import system, it could be the loader instance.

I think the audit event covers this, unless you have some way of using 
this context in mind that I can't think of?

Cheers,
Steve


From steve.dower at python.org  Mon Apr  1 13:42:58 2019
From: steve.dower at python.org (Steve Dower)
Date: Mon, 1 Apr 2019 10:42:58 -0700
Subject: [Python-Dev] PEP 578: Python Runtime Audit Hooks
In-Reply-To: <6ded2c50-bf28-1376-7b0c-9cc6839be56b@python.org>
References: <daa13e96-fa07-8703-4d73-234a862baf05@python.org>
 <2cb3740e-ebc2-1839-1d2e-73d1b9f0a445@python.org>
 <ad05e257-d046-bba8-b070-c24c80a1521e@python.org>
 <CADiSq7ftogxBEZMu1suwMhpyBPehPteqU4EJr4YqxprpFYP-Nw@mail.gmail.com>
 <6ded2c50-bf28-1376-7b0c-9cc6839be56b@python.org>
Message-ID: <16bc5fae-47c0-e9f6-da60-e0e46cb84c78@python.org>

On 30Mar2019 0913, Steve Dower wrote:
> On 30Mar.2019 0747, Nick Coghlan wrote:
>> I like this PEP in principle, but the specific "open_for_import" name
>> bothers me a lot, as it implies that "importing" is the only situation
>> where a file will be opened for code execution.
>>
>> If this part of the API were lower down the stack (e.g.
>> "_io.open_for_code_execution") then I think it would make more sense -
>> APIs like tokenize.open(), runpy.run_path(), PyRun_SimpleFile(),
>> shelve, etc, could use that, without having to introduce a dependency
>> on importlib to get access to the functionality.
> 
> It was called "open_for_exec" at one point, though I forget exactly why
> we changed it. But I have no problem with moving it. Something like this?
> 
> PyImport_OpenForImport -> PyIO_OpenForExec
> PyImport_SetOpenForImportHook -> PyIO_SetOpenForExecHook
> importlib.util.open_for_import -> _io.open_for_exec
> 
> Or more in line with Nick's suggestion:
> 
> PyImport_OpenForImport -> PyIO_OpenExecutableCode
> PyImport_SetOpenForImportHook -> PyIO_SetOpenExecutableCodeHook
> importlib.util.open_for_import -> _io.open_executable_code
> 
> I dropped "For", but I don't really care that much about the name. I'd
> be okay dropping either "executable" or "code" as well - I don't really
> have a good sense of which will make people more likely to use this
> correctly.

Looking at what we already have, perhaps putting it under 
"PyFile_OpenForExecute" would make the most sense? We don't currently 
have any public "PyIO" types or functions.

Bikeshedding now, but as I'm the only one really participating in it, I 
think it's allowed :)

Cheers,
Steve


From cs at cskk.id.au  Mon Apr  1 18:35:39 2019
From: cs at cskk.id.au (Cameron Simpson)
Date: Tue, 2 Apr 2019 09:35:39 +1100
Subject: [Python-Dev] Strange umask(?)/st_mode issue
In-Reply-To: <c478fae6-349e-022e-3b70-81470c0a0b5a@python.org>
References: <c478fae6-349e-022e-3b70-81470c0a0b5a@python.org>
Message-ID: <20190401223539.GA47505@cskk.homeip.net>

On 01Apr2019 09:12, Steve Dower <steve.dower at python.org> wrote:
>On 30Mar2019 1130, Gregory P. Smith wrote:
>>I wouldn't expect it to be the case in a CI environment but I 
>>believe a umask can be overridden if the filesystem is mounted and 
>>configured with acls set?? (oh, hah, Ivan just said the same thing)
>
>Yep, it appears this is the case. The Pipelines team got back to me 
>and it seems to be a known issue - the workaround they gave me was to 
>run "sudo setfacl -Rb /home/vsts" at the start, so I've merged that in 
>for now (to master and 3.7).

Could that be done _without_ sudo to just the local directory containing 
the test tar file? If that works then you don't need any nasty 
privileged sudo use (which will just break on platforms without sudo 
anyway).

Cheers,
Cameron Simpson <cs at cskk.id.au>

From steve.dower at python.org  Mon Apr  1 18:44:13 2019
From: steve.dower at python.org (Steve Dower)
Date: Mon, 1 Apr 2019 15:44:13 -0700
Subject: [Python-Dev] Strange umask(?)/st_mode issue
In-Reply-To: <20190401223539.GA47505@cskk.homeip.net>
References: <c478fae6-349e-022e-3b70-81470c0a0b5a@python.org>
 <20190401223539.GA47505@cskk.homeip.net>
Message-ID: <ab393c80-d5c3-e723-43a9-bccd35936813@python.org>

On 01Apr2019 1535, Cameron Simpson wrote:
> On 01Apr2019 09:12, Steve Dower <steve.dower at python.org> wrote:
>> On 30Mar2019 1130, Gregory P. Smith wrote:
>>> I wouldn't expect it to be the case in a CI environment but I believe 
>>> a umask can be overridden if the filesystem is mounted and configured 
>>> with acls set?? (oh, hah, Ivan just said the same thing)
>>
>> Yep, it appears this is the case. The Pipelines team got back to me 
>> and it seems to be a known issue - the workaround they gave me was to 
>> run "sudo setfacl -Rb /home/vsts" at the start, so I've merged that in 
>> for now (to master and 3.7).
> 
> Could that be done _without_ sudo to just the local directory containing 
> the test tar file? If that works then you don't need any nasty 
> privileged sudo use (which will just break on platforms without sudo 
> anyway).

I tried something similar to that and it didn't work. My guess is it's 
to do with the actual mount point? (I also tried without sudo at first, 
and when I didn't work, I tried it with sudo. I hear that's how to 
decide whether you need it or not ;) )

In any case, it only applies to the Azure Pipelines build definition, so 
there aren't any other platforms where it'll be used.

Cheers,
Steve

From cs at cskk.id.au  Mon Apr  1 19:49:29 2019
From: cs at cskk.id.au (Cameron Simpson)
Date: Tue, 2 Apr 2019 10:49:29 +1100
Subject: [Python-Dev] Strange umask(?)/st_mode issue
In-Reply-To: <ab393c80-d5c3-e723-43a9-bccd35936813@python.org>
References: <ab393c80-d5c3-e723-43a9-bccd35936813@python.org>
Message-ID: <20190401234929.GA53667@cskk.homeip.net>

On 01Apr2019 15:44, Steve Dower <steve.dower at python.org> wrote:
>On 01Apr2019 1535, Cameron Simpson wrote:
>>On 01Apr2019 09:12, Steve Dower <steve.dower at python.org> wrote:
>>>On 30Mar2019 1130, Gregory P. Smith wrote:
>>>>I wouldn't expect it to be the case in a CI environment but I 
>>>>believe a umask can be overridden if the filesystem is mounted 
>>>>and configured with acls set?? (oh, hah, Ivan just said the same 
>>>>thing)
>>>
>>>Yep, it appears this is the case. The Pipelines team got back to 
>>>me and it seems to be a known issue - the workaround they gave me 
>>>was to run "sudo setfacl -Rb /home/vsts" at the start, so I've 
>>>merged that in for now (to master and 3.7).
>>
>>Could that be done _without_ sudo to just the local directory 
>>containing the test tar file? If that works then you don't need any 
>>nasty privileged sudo use (which will just break on platforms 
>>without sudo anyway).
>
>I tried something similar to that and it didn't work. My guess is it's 
>to do with the actual mount point? (I also tried without sudo at 
>first, and when I didn't work, I tried it with sudo. I hear that's how 
>to decide whether you need it or not ;) )
>
>In any case, it only applies to the Azure Pipelines build definition, 
>so there aren't any other platforms where it'll be used.

Ok then.

Curious: is the sudo now in the build setup? I'm just thinking that this 
isn't a tarfile specific fix but a "get correct POSIX umask semantics" 
fix, so it should apply to the entire environment.

Or am I naive?

Cheers,
Cameron Simpson <cs at cskk.id.au>

From greg at krypto.org  Mon Apr  1 19:59:43 2019
From: greg at krypto.org (Gregory P. Smith)
Date: Mon, 1 Apr 2019 16:59:43 -0700
Subject: [Python-Dev] Strange umask(?)/st_mode issue
In-Reply-To: <20190401234929.GA53667@cskk.homeip.net>
References: <ab393c80-d5c3-e723-43a9-bccd35936813@python.org>
 <20190401234929.GA53667@cskk.homeip.net>
Message-ID: <CAGE7PNJFPQjKL2NJUFggPFYeBUufisxzEhPnmfJyCt44ew6rOQ@mail.gmail.com>

On Mon, Apr 1, 2019 at 4:49 PM Cameron Simpson <cs at cskk.id.au> wrote:

> On 01Apr2019 15:44, Steve Dower <steve.dower at python.org> wrote:
> >On 01Apr2019 1535, Cameron Simpson wrote:
> >>On 01Apr2019 09:12, Steve Dower <steve.dower at python.org> wrote:
> >>>On 30Mar2019 1130, Gregory P. Smith wrote:
> >>>>I wouldn't expect it to be the case in a CI environment but I
> >>>>believe a umask can be overridden if the filesystem is mounted
> >>>>and configured with acls set?  (oh, hah, Ivan just said the same
> >>>>thing)
> >>>
> >>>Yep, it appears this is the case. The Pipelines team got back to
> >>>me and it seems to be a known issue - the workaround they gave me
> >>>was to run "sudo setfacl -Rb /home/vsts" at the start, so I've
> >>>merged that in for now (to master and 3.7).
> >>
> >>Could that be done _without_ sudo to just the local directory
> >>containing the test tar file? If that works then you don't need any
> >>nasty privileged sudo use (which will just break on platforms
> >>without sudo anyway).
> >
> >I tried something similar to that and it didn't work. My guess is it's
> >to do with the actual mount point? (I also tried without sudo at
> >first, and when I didn't work, I tried it with sudo. I hear that's how
> >to decide whether you need it or not ;) )
> >
> >In any case, it only applies to the Azure Pipelines build definition,
> >so there aren't any other platforms where it'll be used.
>
> Ok then.
>
> Curious: is the sudo now in the build setup? I'm just thinking that this
> isn't a tarfile specific fix but a "get correct POSIX umask semantics"
> fix, so it should apply to the entire environment.
>
> Or am I naive?
>

I'm reading between the lines and assuming we're not the only user of their
CI complaining about this environment change. ;)
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://mail.python.org/pipermail/python-dev/attachments/20190401/ebd27f4d/attachment.html>

From vano at mail.mipt.ru  Mon Apr  1 23:14:00 2019
From: vano at mail.mipt.ru (Ivan Pozdeev)
Date: Tue, 2 Apr 2019 06:14:00 +0300
Subject: [Python-Dev] Strange umask(?)/st_mode issue
In-Reply-To: <ab393c80-d5c3-e723-43a9-bccd35936813@python.org>
References: <c478fae6-349e-022e-3b70-81470c0a0b5a@python.org>
 <20190401223539.GA47505@cskk.homeip.net>
 <ab393c80-d5c3-e723-43a9-bccd35936813@python.org>
Message-ID: <67704a69-45aa-47dc-8b97-e18e886752bf@mail.mipt.ru>

On 02.04.2019 1:44, Steve Dower wrote:
> On 01Apr2019 1535, Cameron Simpson wrote:
>> On 01Apr2019 09:12, Steve Dower <steve.dower at python.org> wrote:
>>> On 30Mar2019 1130, Gregory P. Smith wrote:
>>>> I wouldn't expect it to be the case in a CI environment but I believe a umask can be overridden if the filesystem is mounted and 
>>>> configured with acls set? (oh, hah, Ivan just said the same thing)
>>>
>>> Yep, it appears this is the case. The Pipelines team got back to me and it seems to be a known issue - the workaround they gave me was 
>>> to run "sudo setfacl -Rb /home/vsts" at the start, so I've merged that in for now (to master and 3.7).
>>
>> Could that be done _without_ sudo to just the local directory containing the test tar file? If that works then you don't need any nasty 
>> privileged sudo use (which will just break on platforms without sudo anyway).
>
> I tried something similar to that and it didn't work. My guess is it's to do with the actual mount point? (I also tried without sudo at 
> first, and when I didn't work, I tried it with sudo. I hear that's how to decide whether you need it or not ;) )
>
> In any case, it only applies to the Azure Pipelines build definition, so there aren't any other platforms where it'll be used.
>
https://github.com/python/cpython/pull/12655
> Cheers,
> Steve
> _______________________________________________
> Python-Dev mailing list
> Python-Dev at python.org
> https://mail.python.org/mailman/listinfo/python-dev
> Unsubscribe: https://mail.python.org/mailman/options/python-dev/vano%40mail.mipt.ru

-- 
Regards,
Ivan


From Peixing.Xin at windriver.com  Tue Apr  2 05:46:14 2019
From: Peixing.Xin at windriver.com (Xin, Peixing)
Date: Tue, 2 Apr 2019 09:46:14 +0000
Subject: [Python-Dev] =?windows-1252?q?how_to_rerun_the_job_=93Azure_Pipe?=
 =?windows-1252?q?lines_PR=94=3F?=
Message-ID: <8488FBC4EAAC5941BA4B85DD1ECCF1870133BAAF35@ALA-MBD.corp.ad.wrs.com>

Hi, Experts:

Anyone can tell how to rerun the job ?Azure Pipelines PR? for my PR? Sometimes my PR failed but this is caused by externals. The next day this external issue was fixed then I might want to rerun this specific job on my PR to get the new result. How can I reach this?

[cid:image001.png at 01D4E97B.F67B1E20]

Thanks,
Peixing

-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://mail.python.org/pipermail/python-dev/attachments/20190402/86d08742/attachment.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 4252 bytes
Desc: image001.png
URL: <http://mail.python.org/pipermail/python-dev/attachments/20190402/86d08742/attachment.png>

From tir.karthi at gmail.com  Tue Apr  2 08:22:56 2019
From: tir.karthi at gmail.com (Karthikeyan)
Date: Tue, 2 Apr 2019 17:52:56 +0530
Subject: [Python-Dev] 
	=?utf-8?q?how_to_rerun_the_job_=E2=80=9CAzure_Pipel?=
	=?utf-8?b?aW5lcyBQUuKAnT8=?=
In-Reply-To: <8488FBC4EAAC5941BA4B85DD1ECCF1870133BAAF35@ALA-MBD.corp.ad.wrs.com>
References: <8488FBC4EAAC5941BA4B85DD1ECCF1870133BAAF35@ALA-MBD.corp.ad.wrs.com>
Message-ID: <CAAjsFLQJ+jDk4N+h=pct+r2W4R87dPZ5uDacJvCQ2ZdbedE5Bw@mail.gmail.com>

Closing and re-opening the PR will trigger the CI run again that might help
in this case but it will run all the jobs.

-- 
Regards,
Karthikeyan S
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://mail.python.org/pipermail/python-dev/attachments/20190402/582c7a43/attachment-0001.html>
-------------- next part --------------
A non-text attachment was scrubbed...
Name: image001.png
Type: image/png
Size: 4252 bytes
Desc: not available
URL: <http://mail.python.org/pipermail/python-dev/attachments/20190402/582c7a43/attachment-0001.png>

From cspealma at redhat.com  Tue Apr  2 11:17:20 2019
From: cspealma at redhat.com (Calvin Spealman)
Date: Tue, 2 Apr 2019 11:17:20 -0400
Subject: [Python-Dev] PEP-582 and multiple Python installations
Message-ID: <CACo5Rz4X+d7mQ1EH=w4geg2q=PMVvrnm3L2sD5+wyd42B_LvsQ@mail.gmail.com>

 (I originally posted this to python-ideas, where I was told none of this
PEP's authors subscribe so probably no one will see it there, so I'm
posting it here to raise the issue where it can get seen and hopefully
discussed)

While the PEP does show the version number as part of the path to the
actual packages, implying support for multiple versions, this doesn't seem
to be spelled out in the actual text. Presumably __pypackages__/3.8/ might
sit beside __pypackages__/3.9/, etc. to keep future versions capable of
installing packages for each version, the way virtualenv today is bound to
one version of Python.

I'd like to raise a potential edge case that might be a problem, and likely
an increasingly common one: users with multiple installations of the *same*
version of Python. This is actually a common setup for Windows users who
use WSL, Microsoft's Linux-on-Windows solution, as you could have both the
Windows and Linux builds of a given Python version installed on the same
machine. The currently implied support for multiple versions would not be
able to separate these and could create problems if users pip install a
Windows binary package through Powershell and then try to run a script in
Bash from the same directory, causing the Linux version of Python to try to
use Windows python packages.

I'm not actually sure what the solution here is. Mostly I wanted to raise
the concern, because I'm very keen on WSL being a great entry path for new
developers and I want to make that a better experience, not a more
confusing one. Maybe that version number could include some other unique
identify, maybe based on Python's own executable. A hash maybe? I don't
know if anything like that already exists to uniquely identify a Python
build or installation.


-- 

CALVIN SPEALMAN

SENIOR QUALITY ENGINEER

cspealma at redhat.com  M: +1.336.210.5107
<https://red.ht/sig>
TRIED. TESTED. TRUSTED. <https://redhat.com/trusted>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: <http://mail.python.org/pipermail/python-dev/attachments/20190402/fee4fe99/attachment.html>

From steve.dower at python.org  Tue Apr  2 12:09:30 2019
From: steve.dower at python.org (Steve Dower)
Date: Tue, 2 Apr 2019 09:09:30 -0700
Subject: [Python-Dev] 
 =?utf-8?q?how_to_rerun_the_job_=E2=80=9CAzure_Pipel?=
 =?utf-8?b?aW5lcyBQUuKAnT8=?=
In-Reply-To: <CAAjsFLQJ+jDk4N+h=pct+r2W4R87dPZ5uDacJvCQ2ZdbedE5Bw@mail.gmail.com>
References: <8488FBC4EAAC5941BA4B85DD1ECCF1870133BAAF35@ALA-MBD.corp.ad.wrs.com>
 <CAAjsFLQJ+jDk4N+h=pct+r2W4R87dPZ5uDacJvCQ2ZdbedE5Bw@mail.gmail.com>
Message-ID: <05055b7d-922f-2b93-4425-4068564313ad@python.org>

On 02Apr2019 0522, Karthikeyan wrote:
> Closing and re-opening the PR will trigger the CI run again that might 
> help in this case but it will run all the jobs.

Yes, I believe this is still the best way to re-run Pipelines jobs.

For people with logins (not yet everyone in the GitHub org, but I hear 
that's coming) you can requeue the build, but last time I tried it 
didn't sync back to the pull request properly (I think it needs GitHub 
to cooperate, which is why triggering it from GitHub works best.)

The Pipelines team is aware of this and working on it, so I expect the 
integration to improve over time. For now, close/reopen the PR.

Cheers,
Steve

From steve.dower at python.org  Tue Apr  2 12:10:59 2019
From: steve.dower at python.org (Steve Dower)
Date: Tue, 2 Apr 2019 09:10:59 -0700
Subject: [Python-Dev] PEP-582 and multiple Python installations
In-Reply-To: <CACo5Rz4X+d7mQ1EH=w4geg2q=PMVvrnm3L2sD5+wyd42B_LvsQ@mail.gmail.com>
References: <CACo5Rz4X+d7mQ1EH=w4geg2q=PMVvrnm3L2sD5+wyd42B_LvsQ@mail.gmail.com>
Message-ID: <2b889555-db6f-6c69-0347-ebb89d6fec21@python.org>

On 02Apr2019 0817, Calvin Spealman wrote:
> (I originally posted this to python-ideas, where I was told none of this 
> PEP's authors subscribe so probably no one will see it there, so I'm 
> posting it here to raise the issue where it can get seen and hopefully 
> discussed)

Correct, thanks for posting. (I thought we had a "discussions-to" tag 
with distutils-sig on it, but apparently not.)

> While the PEP does show the version number as part of the path to the 
> actual packages, implying support for multiple versions, this doesn't 
> seem to be spelled out in the actual text. Presumably 
> __pypackages__/3.8/ might sit beside __pypackages__/3.9/, etc. to keep 
> future versions capable of installing packages for each version, the way 
> virtualenv today is bound to one version of Python.
> 
> I'd like to raise a potential edge case that might be a problem, and 
> likely an increasingly common one: users with multiple installations of 
> the *same* version of Python. This is actually a common setup for 
> Windows users who use WSL, Microsoft's Linux-on-Windows solution, as you 
> could have both the Windows and Linux builds of a given Python version 
> installed on the same machine. The currently implied support for 
> multiple versions would not be able to separate these and could create 
> problems if users pip install a Windows binary package through 
> Powershell and then try to run a script in Bash from the same directory, 
> causing the Linux version of Python to try to use Windows python packages.
> 
> I'm not actually sure what the solution here is. Mostly I wanted to 
> raise the concern, because I'm very keen on WSL being a great entry path 
> for new developers and I want to make that a better experience, not a 
> more confusing one. Maybe that version number could include some other 
> unique identify, maybe based on Python's own executable. A hash maybe? I 
> don't know if anything like that already exists to uniquely identify a 
> Python build or installation.

Yes, this is a situation we're aware of, and it's caught in the conflict 
of "who is this feature meant to support".

Since all platforms have a unique extension module suffix (e.g. 
"module.cp38-win32.pyd"), it would be possible to support this with 
"fat" packages that include all binaries (or some clever way of merging 
wheels for multiple platforms).

And since this is already in CPython itself, it leads to about the only 
reasonable solution - instead of "3.8", use the extension module suffix 
"cp38-win32". (Wheel tags are not in core CPython, so we can't use those.)

But while this seems obvious, it also reintroduces problems that this 
has the potential to fix - suddenly, just like installing into your 
global environment, your packages are not project-specific anymore but 
are Python-specific. Which is one of the major confusions people run 
into ("I pip installed X but now can't import it in python").

So the main points of discussion right now are "whose problem does this 
solve" and "when do we tell people they need a full venv". And that 
discussion is mostly happening at 
https://discuss.python.org/t/pep-582-python-local-packages-directory/963/

Cheers,
Steve

From pviktori at redhat.com  Tue Apr  2 08:49:56 2019
From: pviktori at redhat.com (Petr Viktorin)
Date: Tue, 2 Apr 2019 14:49:56 +0200
Subject: [Python-Dev] PEP 590 discussion
In-Reply-To: <5C9FEF82.50207@UGent.be>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
Message-ID: <d28319ff-a8aa-f218-b8ed-1d489ff9e324@redhat.com>

On 3/30/19 11:36 PM, Jeroen Demeyer wrote:
> On 2019-03-30 17:30, Mark Shannon wrote:
>> 2. The claim that PEP 580 allows "certain optimizations because other
>> code can make assumptions" is flawed. In general, the caller cannot make
>> assumptions about the callee or vice-versa. Python is a dynamic language.
> 
> PEP 580 is meant for extension classes, not Python classes. Extension 
> classes are not dynamic. When you implement tp_call in a given way, the 
> user cannot change it. So if a class implements the C call protocol or 
> the vectorcall protocol, callers can make assumptions about what that 
> means.
> 
>> PEP 579 is mainly a list of supposed flaws with the
>> 'builtin_function_or_method' class.
>> The general thrust of PEP 579 seems to be that builtin-functions and
>> builtin-methods should be more flexible and extensible than they are. I
>> don't agree. If you want different behaviour, then use a different
>> object. Don't try an cram all this extra behaviour into a pre-existing
>> object.
> 
> I think that there is a misunderstanding here. I fully agree with the 
> "use a different object" solution. This isn't a new solution: it's 
> already possible to implement those different objects (Cython does it). 
> It's just that this solution comes at a performance cost and that's what 
> we want to avoid.

It does seem like there is some misunderstanding.

PEP 580 defines a CCall structure, which includes the function pointer, 
flags, "self" and "parent". Like the current implementation, it has 
various METH_ flags for various C signatures. When called, the info from 
CCall is matched up (in relatively complex ways) to what the C function 
expects.

PEP 590 only adds the "vectorcall". It does away with flags and only has 
one C signatures, which is designed to fit all the existing ones, and is 
well optimized. Storing the "self"/"parent", and making sure they're 
passed to the C function is the responsibility of the callable object.
There's an optimization for "self" (offsetting using 
PY_VECTORCALL_ARGUMENTS_OFFSET), and any supporting info can be provided 
as part of "self".

>> I'll reiterate that PEP 590 is more general than PEP 580 and that once
>> the callable's code has access to the callable object (as both PEPs
>> allow) then anything is possible. You can't can get more extensible than
>> that.

Anything is possible, but if one of the possibilities becomes common and 
useful, PEP 590 would make it hard to optimize for it.
Python has grown many "METH_*" signatures over the years as we found 
more things that need to be passed to callables. Why would 
"METH_VECTORCALL" be the last? If it won't (if you think about it as one 
more way to call functions), then dedicating a tp_* slot to it sounds 
quite expensive.


In one of the ways to call C functions in PEP 580, the function gets 
access to:
- the arguments,
- "self", the object
- the class that the method was found in (which is not necessarily 
type(self))
I still have to read the details, but when combined with 
LOAD_METHOD/CALL_METHOD optimization (avoiding creation of a "bound 
method" object), it seems impossible to do this efficiently with just 
the callable's code and callable's object.


> I would argue the opposite: PEP 590 defines a fixed protocol that is not 
> easy to extend. PEP 580 on the other hand uses a new data structure 
> PyCCallDef which could easily be extended in the future (this will 
> intentionally never be part of the stable ABI, so we can do that).
> 
> I have also argued before that the generality of PEP 590 is a bad thing 
> rather than a good thing: by defining a more rigid protocol as in PEP 
> 580, more optimizations are possible.
>
>> PEP 580 has the same limitation for the same reasons. The limitation is
>> necessary for correctness if an object supports calls via `__call__` and
>> through another calling convention.
> 
> I don't think that this limitation is needed in either PEP. As I 
> explained at the top of this email, it can easily be solved by not using 
> the protocol for Python classes. What is wrong with my proposal in PEP 
> 580: https://www.python.org/dev/peps/pep-0580/#inheritance


I'll add Jeroen's notes from the review of the proposed PEP 590
(https://github.com/python/peps/pull/960):

The statement "PEP 580 is specifically targetted at function-like 
objects, and doesn't support other callables like classes, partial 
functions, or proxies" is factually false. The motivation for PEP 580 is 
certainly function/method-like objects but it's a general protocol that 
every class can implement. For certain classes, it may not be easy or 
desirable to do that but it's always possible.

Given that `PY_METHOD_DESCRIPTOR` is a flag for tp_flags, shouldn't it 
be called `Py_TPFLAGS_METHOD_DESCRIPTOR` or something?

Py_TPFLAGS_HAVE_VECTOR_CALL should be Py_TPFLAGS_HAVE_VECTORCALL, to be 
consistent with tp_vectorcall_offset and other uses of "vectorcall" (not 
"vector call")


And mine, so far:

I'm not clear on the constness of the "args" array.
If it is mutable (PyObject **), you can't, for example, directly pass a 
tuple's storage (or any other array that could be used in the call).
If it is not (PyObject * const *), you can't insert the "self" argument in.
The reference implementations seems to be inconsistent here. What's the 
intention?


From mark at hotpy.org  Tue Apr  2 15:38:23 2019
From: mark at hotpy.org (Mark Shannon)
Date: Tue, 2 Apr 2019 20:38:23 +0100
Subject: [Python-Dev] PEP 580/590 discussion
In-Reply-To: <5CA1A217.1030007@UGent.be>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
 <5CA1A217.1030007@UGent.be>
Message-ID: <ce2d2166-b928-2ee5-18a1-71215daafa83@hotpy.org>

Hi,

On 01/04/2019 6:31 am, Jeroen Demeyer wrote:
> I added benchmarks for PEP 590:
> 
> https://gist.github.com/jdemeyer/f0d63be8f30dc34cc989cd11d43df248

Thanks. As expected for calls to C function for both PEPs and master 
perform about the same, as they are using almost the same calling 
convention under the hood.

As an example of the advantage that a general fast calling convention 
gives you, I have implemented the vectorcall versions of list() and range()

https://github.com/markshannon/cpython/compare/vectorcall-minimal...markshannon:vectorcall-examples

Which gives a roughly 30% reduction in time for creating ranges, or 
lists from small tuples.

https://gist.github.com/markshannon/5cef3a74369391f6ef937d52cca9bfc8

Cheers,
Mark.

From mark at hotpy.org  Tue Apr  2 17:12:11 2019
From: mark at hotpy.org (Mark Shannon)
Date: Tue, 2 Apr 2019 22:12:11 +0100
Subject: [Python-Dev] PEP 590 discussion
In-Reply-To: <d28319ff-a8aa-f218-b8ed-1d489ff9e324@redhat.com>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
 <d28319ff-a8aa-f218-b8ed-1d489ff9e324@redhat.com>
Message-ID: <421f8182-4bc8-b8cf-82d6-ca4a4fbd2013@hotpy.org>

Hi,

On 02/04/2019 1:49 pm, Petr Viktorin wrote:
> On 3/30/19 11:36 PM, Jeroen Demeyer wrote:
>> On 2019-03-30 17:30, Mark Shannon wrote:
>>> 2. The claim that PEP 580 allows "certain optimizations because other
>>> code can make assumptions" is flawed. In general, the caller cannot make
>>> assumptions about the callee or vice-versa. Python is a dynamic 
>>> language.
>>
>> PEP 580 is meant for extension classes, not Python classes. Extension 
>> classes are not dynamic. When you implement tp_call in a given way, 
>> the user cannot change it. So if a class implements the C call 
>> protocol or the vectorcall protocol, callers can make assumptions 
>> about what that means.
>>
>>> PEP 579 is mainly a list of supposed flaws with the
>>> 'builtin_function_or_method' class.
>>> The general thrust of PEP 579 seems to be that builtin-functions and
>>> builtin-methods should be more flexible and extensible than they are. I
>>> don't agree. If you want different behaviour, then use a different
>>> object. Don't try an cram all this extra behaviour into a pre-existing
>>> object.
>>
>> I think that there is a misunderstanding here. I fully agree with the 
>> "use a different object" solution. This isn't a new solution: it's 
>> already possible to implement those different objects (Cython does 
>> it). It's just that this solution comes at a performance cost and 
>> that's what we want to avoid.
> 
> It does seem like there is some misunderstanding.
> 
> PEP 580 defines a CCall structure, which includes the function pointer, 
> flags, "self" and "parent". Like the current implementation, it has 
> various METH_ flags for various C signatures. When called, the info from 
> CCall is matched up (in relatively complex ways) to what the C function 
> expects.
> 
> PEP 590 only adds the "vectorcall". It does away with flags and only has 
> one C signatures, which is designed to fit all the existing ones, and is 
> well optimized. Storing the "self"/"parent", and making sure they're 
> passed to the C function is the responsibility of the callable object.
> There's an optimization for "self" (offsetting using 
> PY_VECTORCALL_ARGUMENTS_OFFSET), and any supporting info can be provided 
> as part of "self". >
>>> I'll reiterate that PEP 590 is more general than PEP 580 and that once
>>> the callable's code has access to the callable object (as both PEPs
>>> allow) then anything is possible. You can't can get more extensible than
>>> that.
> 
> Anything is possible, but if one of the possibilities becomes common and 
> useful, PEP 590 would make it hard to optimize for it.
> Python has grown many "METH_*" signatures over the years as we found 
> more things that need to be passed to callables. Why would 
> "METH_VECTORCALL" be the last? If it won't (if you think about it as one 
> more way to call functions), then dedicating a tp_* slot to it sounds 
> quite expensive.

I doubt METH_VECTORCALL will be the last.
Let me give you an example: It is quite common for a function to take 
two arguments, so we might want add a METH_OO flag for builtin-functions 
with 2 parameters.

To support this in PEP 590, you would make exactly the same change as 
you would now; which is to add another case to the switch statement in 
_PyCFunction_FastCallKeywords.
For PEP 580, you would add another case to the switch in PyCCall_FastCall.

No difference really.

PEP 580 uses a slot as well. It's only 8 bytes per class.

> 
> 
> In one of the ways to call C functions in PEP 580, the function gets 
> access to:
> - the arguments,
> - "self", the object
> - the class that the method was found in (which is not necessarily 
> type(self))
> I still have to read the details, but when combined with 
> LOAD_METHOD/CALL_METHOD optimization (avoiding creation of a "bound 
> method" object), it seems impossible to do this efficiently with just 
> the callable's code and callable's object.

It is possible, and relatively straightforward.
Why do you think it is impossible?

> 
> 
>> I would argue the opposite: PEP 590 defines a fixed protocol that is 
>> not easy to extend. PEP 580 on the other hand uses a new data 
>> structure PyCCallDef which could easily be extended in the future 
>> (this will intentionally never be part of the stable ABI, so we can do 
>> that).
>>
>> I have also argued before that the generality of PEP 590 is a bad 
>> thing rather than a good thing: by defining a more rigid protocol as 
>> in PEP 580, more optimizations are possible.
>>
>>> PEP 580 has the same limitation for the same reasons. The limitation is
>>> necessary for correctness if an object supports calls via `__call__` and
>>> through another calling convention.
>>
>> I don't think that this limitation is needed in either PEP. As I 
>> explained at the top of this email, it can easily be solved by not 
>> using the protocol for Python classes. What is wrong with my proposal 
>> in PEP 580: https://www.python.org/dev/peps/pep-0580/#inheritance
> 
> 
> I'll add Jeroen's notes from the review of the proposed PEP 590
> (https://github.com/python/peps/pull/960):
> 
> The statement "PEP 580 is specifically targetted at function-like 
> objects, and doesn't support other callables like classes, partial 
> functions, or proxies" is factually false. The motivation for PEP 580 is 
> certainly function/method-like objects but it's a general protocol that 
> every class can implement. For certain classes, it may not be easy or 
> desirable to do that but it's always possible. >
> Given that `PY_METHOD_DESCRIPTOR` is a flag for tp_flags, shouldn't it 
> be called `Py_TPFLAGS_METHOD_DESCRIPTOR` or something?
> 
> Py_TPFLAGS_HAVE_VECTOR_CALL should be Py_TPFLAGS_HAVE_VECTORCALL, to be 
> consistent with tp_vectorcall_offset and other uses of "vectorcall" (not 
> "vector call")
> 

Thanks for the comments, I'll update the PEP when I get the chance.

> 
> And mine, so far:
> 
> I'm not clear on the constness of the "args" array.
> If it is mutable (PyObject **), you can't, for example, directly pass a 
> tuple's storage (or any other array that could be used in the call).
> If it is not (PyObject * const *), you can't insert the "self" argument in.
> The reference implementations seems to be inconsistent here. What's the 
> intention?
> 

I'll make it clearer in the PEP.
My thinking was that if `PY_VECTORCALL_ARGUMENTS_OFFSET` is set then the 
caller is allowing the callee to mutate element -1.
It would make sense to generalise that to any element of the vector 
(including -1).
When passing the contents of a tuple, `PY_VECTORCALL_ARGUMENTS_OFFSET` 
should not be set, and thus the vector could not be mutated.


Cheers,
Mark.

From J.Demeyer at UGent.be  Wed Apr  3 01:33:49 2019
From: J.Demeyer at UGent.be (Jeroen Demeyer)
Date: Wed, 3 Apr 2019 07:33:49 +0200
Subject: [Python-Dev] PEP 590 discussion
In-Reply-To: <421f8182-4bc8-b8cf-82d6-ca4a4fbd2013@hotpy.org>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
 <d28319ff-a8aa-f218-b8ed-1d489ff9e324@redhat.com>
 <421f8182-4bc8-b8cf-82d6-ca4a4fbd2013@hotpy.org>
Message-ID: <5CA445BD.4040705@UGent.be>

>> In one of the ways to call C functions in PEP 580, the function gets
>> access to:
>> - the arguments,
>> - "self", the object
>> - the class that the method was found in (which is not necessarily
>> type(self))
>> I still have to read the details, but when combined with
>> LOAD_METHOD/CALL_METHOD optimization (avoiding creation of a "bound
>> method" object), it seems impossible to do this efficiently with just
>> the callable's code and callable's object.
>
> It is possible, and relatively straightforward.

Access to the class isn't possible currently and also not with PEP 590. 
But it's easy enough to fix that: PEP 573 adds a new METH_METHOD flag to 
change the signature of the C function (not the vectorcall wrapper). PEP 
580 supports this "out of the box" because I'm reusing the class also to 
do type checks. But this shouldn't be an argument for or against either PEP.

From J.Demeyer at UGent.be  Wed Apr  3 01:43:28 2019
From: J.Demeyer at UGent.be (Jeroen Demeyer)
Date: Wed, 3 Apr 2019 07:43:28 +0200
Subject: [Python-Dev] PEP 580/590 discussion
In-Reply-To: <ce2d2166-b928-2ee5-18a1-71215daafa83@hotpy.org>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
 <5CA1A217.1030007@UGent.be> <ce2d2166-b928-2ee5-18a1-71215daafa83@hotpy.org>
Message-ID: <5CA44800.9050901@UGent.be>

On 2019-04-02 21:38, Mark Shannon wrote:
> Hi,
>
> On 01/04/2019 6:31 am, Jeroen Demeyer wrote:
>> I added benchmarks for PEP 590:
>>
>> https://gist.github.com/jdemeyer/f0d63be8f30dc34cc989cd11d43df248
>
> Thanks. As expected for calls to C function for both PEPs and master
> perform about the same, as they are using almost the same calling
> convention under the hood.

While they are "about the same", in general PEP 580 is slightly faster 
than master and PEP 590. And PEP 590 actually has a minor slow-down for 
METH_VARARGS calls.

I think that this happens because PEP 580 has less levels of indirection 
than PEP 590. The vectorcall protocol (PEP 590) changes a slower level 
(tp_call) by a faster level (vectorcall), while PEP 580 just removes 
that level entirely: it calls the C function directly.

This shows that PEP 580 is really meant to have maximal performance in 
all cases, accidentally even making existing code faster.


Jeroen.

From J.Demeyer at UGent.be  Wed Apr  3 11:41:06 2019
From: J.Demeyer at UGent.be (Jeroen Demeyer)
Date: Wed, 3 Apr 2019 17:41:06 +0200
Subject: [Python-Dev] PEP 590 vs. bpo-29259
In-Reply-To: <5CA445BD.4040705@UGent.be>
References: <CADiSq7f7GRJjZmoLAxpN5hovMT5=M73aJUi14HHFDNG=hssV7w@mail.gmail.com>
 <6c8356e3-f9b2-c39e-63c4-17f146d326b7@hotpy.org>
 <CA+=+wqBsUKWHRqjhRFShmY77vaxDPTFaafWtOB8dnyXQSthiZw@mail.gmail.com>
 <15b8a3d7-00ed-a5eb-475c-a3adee671b5f@hotpy.org> <5C9FEF82.50207@UGent.be>
 <d28319ff-a8aa-f218-b8ed-1d489ff9e324@redhat.com>
 <421f8182-4bc8-b8cf-82d6-ca4a4fbd2013@hotpy.org> <5CA445BD.4040705@UGent.be>
Message-ID: <5CA4D412.7050906@UGent.be>

As I'm reading the PEP 590 reference implementation, it strikes me how 
similar it is to https://bugs.python.org/issue29259

The main difference is that bpo-29259 has a per-class pointer 
tp_fastcall instead of a per-object pointer. But actually, the PEP 590 
reference implementation does not make much use of the per-object 
pointer: for all classes except "type", the vectorcall wrapper is the 
same for all objects of a given type.

One thing that bpo-29259 did not realize is that existing optimizations 
could be dropped in favor of using tp_fastcall. For example, bpo-29259 
has code like

     if (PyFunction_Check(callable)) {
         return _PyFunction_FastCallKeywords(...);
     }
     if (PyCFunction_Check(callable)) {
         return _PyCFunction_FastCallKeywords(...);
     }
     else if (PyType_HasFeature(..., Py_TPFLAGS_HAVE_FASTCALL) ...)

but the first 2 branches are superfluous given the third.

Anyway, this is just putting PEP 590 a bit in perspective. It doesn't 
say anything about the merits of PEP 590.


Jeroen.

